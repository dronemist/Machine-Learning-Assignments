{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import nltk\n",
    "import time\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_index = 0\n",
    "tweet_index = 5\n",
    "_alpha = 1\n",
    "data = pandas.read_csv('./data/training.csv', encoding='latin-1', header = None).values\n",
    "total_documents = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing into positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = data[:, tweet_index]\n",
    "positive_tweets = data[data[:, polarity_index] == 4, tweet_index]\n",
    "negative_tweets = data[data[:, polarity_index] == 0, tweet_index]\n",
    "all_training_polarity = data[:, polarity_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = lambda x: re.sub(\"[^\\w]\", \" \",  x).split()\n",
    "split = lambda x: re.split(\" |,|\\.\", x)\n",
    "# split = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_ = list(map(split, all_tweets.tolist()))\n",
    "all_words = []\n",
    "for w in all_words_:\n",
    "    all_words += w\n",
    "total_vocabulary = len(set(all_words))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 4\n",
    "positive_words_ = list(map(split, positive_tweets.tolist()))\n",
    "positive_words = []\n",
    "for w in positive_words_:\n",
    "    positive_words += w\n",
    "total_words_positive_tweets = len(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 0\n",
    "negative_words_ = list(map(split, negative_tweets.tolist()))\n",
    "negative_words = []\n",
    "for w in negative_words_:\n",
    "    negative_words += w\n",
    "total_words_negative_tweets = len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tweets_counter = Counter(positive_words)\n",
    "n_tweets_counter = Counter(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting if a sentence is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    \"\"\"\n",
    "    Make a prediction on a given entry\n",
    "    \"\"\"\n",
    "    # P(x|y = 0) and P(x|y = 4)  \n",
    "    p_x_y_0 = 0\n",
    "    p_x_y_4 = 0\n",
    "    \n",
    "    # P(y = 0) and P(y = 4)\n",
    "    p_y_0 = math.log(negative_tweets.shape[0] / total_documents)\n",
    "    p_y_4 = math.log(positive_tweets.shape[0] / total_documents)\n",
    "    \n",
    "    for word in sentence:\n",
    "        p_x_y_0 += (math.log(n_tweets_counter[word] + _alpha) - math.log(total_words_negative_tweets + _alpha * total_vocabulary))\n",
    "        p_x_y_4 += (math.log(p_tweets_counter[word] + _alpha) - math.log(total_words_positive_tweets + _alpha * total_vocabulary))\n",
    "        \n",
    "    # P(y = 0|x) and P(y = 4|x)    \n",
    "    p_y_0_x = p_x_y_0 + p_y_0\n",
    "    p_y_4_x = p_x_y_4 + p_y_4\n",
    "    \n",
    "    predicted_polarity = 4\n",
    "    if p_y_0_x > p_y_4_x:\n",
    "        predicted_polarity = 0\n",
    "    return predicted_polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy over training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from Naive Bayes obtained is 84.93 \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total_count = 0\n",
    "for (i, tweet) in enumerate(all_words_):\n",
    "\n",
    "    # Current Prediction    \n",
    "    prediction = predict(tweet)\n",
    "    if prediction == all_training_polarity[i]:\n",
    "        count += 1\n",
    "    total_count += 1\n",
    "\n",
    "print(\"The accuracy from Naive Bayes obtained is {} \".format(count / total_count * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pandas.read_csv('./data/testdata.csv', encoding='latin-1', header = None).values\n",
    "all_test_tweets = test_data[test_data[:, polarity_index] != 2, tweet_index]\n",
    "all_polarity = test_data[test_data[:, polarity_index] != 2, polarity_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_words_ = list(map(split, all_test_tweets.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from Naive Bayes obtained is 81.05849582172702 \n"
     ]
    }
   ],
   "source": [
    "# Actual prediction\n",
    "def get_accuracy():\n",
    "    \n",
    "    predicted_polarity = np.zeros_like(all_polarity)\n",
    "    count = 0\n",
    "    total_count = 0\n",
    "    for (i, tweet) in enumerate(all_test_words_):\n",
    "\n",
    "        # Current Prediction    \n",
    "        prediction = predict(tweet)\n",
    "        predicted_polarity[i] = prediction\n",
    "        if prediction == all_polarity[i]:\n",
    "            count += 1\n",
    "        total_count += 1\n",
    "\n",
    "    print(\"The accuracy from Naive Bayes obtained is {} \".format(count / total_count * 100))\n",
    "    return predicted_polarity\n",
    "\n",
    "predicted_polarity = get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from Random prediction is 52.36768802228412 \n"
     ]
    }
   ],
   "source": [
    "# Random prediction\n",
    "count = 0\n",
    "total_count = 0\n",
    "for polarity in all_polarity:\n",
    "    # Random choice between 0 and 4\n",
    "    prediction = random.choice([0, 4])\n",
    "    if prediction == polarity:\n",
    "        count += 1\n",
    "    total_count += 1\n",
    "    \n",
    "print(\"The accuracy from Random prediction is {} \".format(count / total_count * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from majority prediction is 49.30362116991643 \n"
     ]
    }
   ],
   "source": [
    "# Majority prediction\n",
    "count = 0\n",
    "total_count = 0\n",
    "\n",
    "if negative_tweets.shape[0] >= positive_tweets.shape[0]:\n",
    "    prediction = 0\n",
    "else:\n",
    "    prediction = 4\n",
    "    \n",
    "for polarity in all_polarity:\n",
    "    if prediction == polarity:\n",
    "        count += 1\n",
    "    total_count += 1\n",
    "\n",
    "print(\"The accuracy from majority prediction is {} \".format(count / total_count * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      4\n",
       "0  143.0   34.0\n",
       "4   34.0  148.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix = np.zeros((2, 2))\n",
    "for (i, prediction) in enumerate(predicted_polarity):\n",
    "    confusion_matrix[int(prediction / 4), int(all_polarity[i] / 4)] += 1\n",
    "j = {\n",
    "    \"0\": confusion_matrix[:, 0],\n",
    "    \"4\": confusion_matrix[:, 1]\n",
    "}\n",
    "df = pandas.DataFrame(j)\n",
    "df.index = ['0', '4']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Stemming and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "tknzr = TweetTokenizer(strip_handles=True)\n",
    "def stem_and_tokenize(sentence):\n",
    "    words = tknzr.tokenize(sentence)\n",
    "    processed_words = []\n",
    "    for w in words:\n",
    "        processed_words += [ps.stem(w.lower())]\n",
    "    processed_words = [w for w in processed_words if not w in stop_words]\n",
    "    return processed_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 543 ms, total: 2min 39s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Y = 4\n",
    "positive_words_ = list(map(stem_and_tokenize, positive_tweets.tolist()))\n",
    "positive_words = []\n",
    "for w in positive_words_:\n",
    "    positive_words += w\n",
    "total_words_positive_tweets = len(positive_words)\n",
    "# for (i, w) in enumerate(positive_words):\n",
    "#     if positive_words[i].startswith('@'):\n",
    "#         positive_words[i] = \"\"\n",
    "#     else:    \n",
    "#         positive_words[i] = ps.stem(w.lower())\n",
    "# positive_words = [w for w in positive_words if w != \"\" and not w in stop_words]\n",
    "# total_words_positive_tweets = len(positive_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 125 ms, total: 2min 39s\n",
      "Wall time: 2min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Y = 0\n",
    "negative_words_ = list(map(stem_and_tokenize, negative_tweets.tolist()))\n",
    "negative_words = []\n",
    "for w in negative_words_:\n",
    "    negative_words += w\n",
    "total_words_negative_tweets = len(negative_words)\n",
    "# for (i, w) in enumerate(negative_words):\n",
    "#     if negative_words[i].startswith('@'):\n",
    "#         negative_words[i] = \"\"\n",
    "#     else:    \n",
    "#         negative_words[i] = ps.stem(w.lower())\n",
    "# negative_words = [w for w in negative_words if w != \"\" and not w in stop_words]    \n",
    "# total_words_negative_tweets = len(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary = len(set(positive_words + negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tweets_counter = Counter(positive_words)\n",
    "n_tweets_counter = Counter(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_words_ = list(map(stem_and_tokenize, all_test_tweets.tolist()))\n",
    "# for (i, w) in enumerate(all_test_words_):\n",
    "#     all_test_words_[i] = list(map(lambda x: ps.stem(x.lower()), all_test_words_[i]))\n",
    "#     all_test_words_[i] = [w for w in all_test_words_[i] if not w.startswith('@')]\n",
    "#     all_test_words_[i] = [w for w in all_test_words_[i] if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from Naive Bayes obtained is 81.61559888579387 \n"
     ]
    }
   ],
   "source": [
    "predicted_polarity = get_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(s):\n",
    "    \"\"\"\n",
    "    Generates bigrams for the string s\n",
    "    \"\"\"\n",
    "    n = 2\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 4\n",
    "# Generating Bigrams\n",
    "positive_bigrams_ = list(map(lambda x: zip(*[x[i:] for i in range(2)]), positive_words_))\n",
    "positive_bigrams_ = [[\" \".join(ngram) for ngram in ngrams_] for ngrams_ in positive_bigrams_]\n",
    "positive_bigrams = []\n",
    "for w in positive_bigrams_:\n",
    "    positive_bigrams += w\n",
    "total_positive_bigrams = len(positive_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = 0\n",
    "negative_bigrams_ = list(map(lambda x: zip(*[x[i:] for i in range(2)]), negative_words_))\n",
    "negative_bigrams_ = [[\" \".join(ngram) for ngram in ngrams_] for ngrams_ in negative_bigrams_]\n",
    "negative_bigrams = []\n",
    "for w in negative_bigrams_:\n",
    "    negative_bigrams += w\n",
    "total_negative_bigrams = len(negative_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bigrams_counter = Counter(positive_bigrams)\n",
    "n_bigrams_counter = Counter(negative_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bigrams = len(set(negative_bigrams + positive_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_predict(sentence, bigrams):\n",
    "    \"\"\"\n",
    "    Make a prediction on a given entry\n",
    "    \"\"\"\n",
    "    # P(x|y = 0) and P(x|y = 4)  \n",
    "    p_x_y_0 = 0\n",
    "    p_x_y_4 = 0\n",
    "    \n",
    "    # P(y = 0) and P(y = 4)\n",
    "    p_y_0 = math.log(negative_tweets.shape[0] / total_documents)\n",
    "    p_y_4 = math.log(positive_tweets.shape[0] / total_documents)\n",
    "    \n",
    "    for word in sentence:\n",
    "        p_x_y_0 += (math.log(n_tweets_counter[word] + _alpha) - math.log(total_words_negative_tweets + _alpha * total_vocabulary))\n",
    "        p_x_y_4 += (math.log(p_tweets_counter[word] + _alpha) - math.log(total_words_positive_tweets + _alpha * total_vocabulary))\n",
    "    \n",
    "    for word in bigrams:\n",
    "        p_x_y_0 += (math.log(n_bigrams_counter[word] + _alpha) - math.log(total_negative_bigrams + _alpha * total_bigrams))\n",
    "        p_x_y_4 += (math.log(p_bigrams_counter[word] + _alpha) - math.log(total_positive_bigrams + _alpha * total_bigrams))\n",
    "        \n",
    "\n",
    "    # P(y = 0|x) and P(y = 4|x)    \n",
    "    p_y_0_x = p_x_y_0 + p_y_0\n",
    "    p_y_4_x = p_x_y_4 + p_y_4\n",
    "    \n",
    "    predicted_polarity = 4\n",
    "    if p_y_0_x > p_y_4_x:\n",
    "        predicted_polarity = 0\n",
    "    return predicted_polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_bigrams_ = list(map(generate_bigrams, all_test_tweets.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy from Naive Bayes obtained is 83.008356545961 \n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total_count = 0\n",
    "for (i, tweet) in enumerate(all_test_words_):\n",
    "\n",
    "    # Current Prediction    \n",
    "    prediction = modified_predict(tweet, all_test_bigrams_[i])\n",
    "    if prediction == all_polarity[i]:\n",
    "        count += 1\n",
    "    total_count += 1\n",
    "\n",
    "print(\"The accuracy from Naive Bayes obtained is {} \".format(count / total_count * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "features_train = vectorizer.fit_transform(all_tweets)\n",
    "features_test = vectorizer.transform(all_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectPercentile(percentile=10, score_func=<function f_classif at 0x1a203586a8>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectPercentile(f_classif, percentile=0.01)\n",
    "selector.fit(features_train, all_training_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = selector.transform(features_train).toarray()\n",
    "features_test = selector.transform(features_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "t0 = time()\n",
    "model = GaussianNB()\n",
    "model.partial_fit(features_train, all_training_polarity.tolist(), np.array([0, 4]))\n",
    "print(f\"Training time: {round(time()-t0, 3)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "score_test = model.score(features_test, all_polarity.tolist())\n",
    "print(f\"Prediction time (test): {round(time()-t0, 3)}s\")\n",
    "print(\"Test set score:\", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_predict_ROC(sentence, bigrams):\n",
    "    \"\"\"\n",
    "    Make a prediction on a given entry\n",
    "    \"\"\"\n",
    "    # P(x|y = 0) and P(x|y = 4)  \n",
    "    p_x_y_0 = 0\n",
    "    p_x_y_4 = 0\n",
    "    \n",
    "    # P(y = 0) and P(y = 4)\n",
    "    p_y_0 = math.log(negative_tweets.shape[0] / total_documents)\n",
    "    p_y_4 = math.log(positive_tweets.shape[0] / total_documents)\n",
    "    \n",
    "    for word in sentence:\n",
    "        p_x_y_0 += (math.log(n_tweets_counter[word] + _alpha) - math.log(total_words_negative_tweets + _alpha * total_vocabulary))\n",
    "        p_x_y_4 += (math.log(p_tweets_counter[word] + _alpha) - math.log(total_words_positive_tweets + _alpha * total_vocabulary))\n",
    "\n",
    "    for word in bigrams:\n",
    "        p_x_y_0 += (math.log(n_bigrams_counter[word] + _alpha) - math.log(total_negative_bigrams + _alpha * total_bigrams))\n",
    "        p_x_y_4 += (math.log(p_bigrams_counter[word] + _alpha) - math.log(total_positive_bigrams + _alpha * total_bigrams))\n",
    "\n",
    "    # P(y = 0|x) and P(y = 4|x)    \n",
    "    p_y_0_x = p_x_y_0 + p_y_0 \n",
    "    p_y_4_x = p_x_y_4 + p_y_4\n",
    "    \n",
    "#     predicted_polarity = 4\n",
    "#     if p_y_0_x > threshold:\n",
    "#         predicted_polarity = 0\n",
    "    return (p_y_0_x - p_y_4_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.array([])\n",
    "for (i, tweet) in enumerate(all_test_words_):\n",
    "\n",
    "    # Current Prediction    \n",
    "    prediction = modified_predict_ROC(tweet, all_test_bigrams_[i])\n",
    "    score = np.append(score, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive_rate = np.array([])\n",
    "false_positive_rate = np.array([])\n",
    "threshold = np.linspace(np.min(score), np.max(score), 100)\n",
    "for t in threshold:\n",
    "    p_count = 0\n",
    "    f_count = 0\n",
    "    for (i, s) in enumerate(score):\n",
    "        if s > t:\n",
    "            if all_polarity[i] == 0:\n",
    "                p_count += 1\n",
    "            else:\n",
    "                f_count += 1\n",
    "    true_positive_rate = np.append(true_positive_rate, p_count)\n",
    "    false_positive_rate = np.append(false_positive_rate, f_count)\n",
    "# fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "# roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "# plt.legend(loc = 'lower right')\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "from cvxopt import solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the SVM data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 9\n",
    "svm_train_data = pandas.read_csv('./data/fashion_mnist/train.csv').values\n",
    "svm_test_data = pandas.read_csv('./data/fashion_mnist/test.csv').values\n",
    "svm_validation_data = pandas.read_csv('./data/fashion_mnist/val.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required data\n",
    "svm_train_data_r = svm_train_data[(svm_train_data[:, -1] == (d + 1) % 10) + (svm_train_data[:, -1] == d), :]\n",
    "svm_test_data_r = svm_test_data[(svm_test_data[:, -1] == (d + 1) % 10) + (svm_test_data[:, -1] == d), :]\n",
    "svm_validation_data_r = svm_validation_data[(svm_validation_data[:, -1] == (d + 1) % 10) + (svm_validation_data[:, -1] == d), :]\n",
    "svm_train_data_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 785)\n",
      "(999, 785)\n",
      "(4500, 785)\n"
     ]
    }
   ],
   "source": [
    "# Reading y's\n",
    "y_train = svm_train_data_r[:, -1]\n",
    "y_train[y_train == d] = -1\n",
    "y_train[y_train == (d + 1) % 10] = 1\n",
    "print(svm_train_data_r.shape)\n",
    "\n",
    "y_test = svm_test_data_r[:, -1]\n",
    "y_test[y_test == d] = -1\n",
    "y_test[y_test == (d + 1) % 10] = 1\n",
    "print(svm_test_data_r.shape)\n",
    "\n",
    "y_val = svm_validation_data_r[:, -1]\n",
    "y_val[y_val == d] = -1\n",
    "y_val[y_val == (d + 1) % 10] = 1\n",
    "print(svm_train_data_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading x's and normalising\n",
    "x_train = (np.delete(svm_train_data_r, -1, axis = 1) / 255).T\n",
    "x_test = (np.delete(svm_test_data_r, -1, axis = 1) / 255).T\n",
    "x_val = (np.delete(svm_validation_data_r, -1, axis = 1) / 255).T\n",
    "\n",
    "# Noise\n",
    "C = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating P matrix\n",
    "y_diag = y_train.reshape((-1, 1))\n",
    "p_matrix = np.matmul(x_train.T, x_train)\n",
    "p_matrix = y_diag * p_matrix\n",
    "y_diag = y_diag.T\n",
    "p_matrix = p_matrix * y_diag\n",
    "p_matrix = matrix(p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating q matrix\n",
    "q_matrix = -matrix(np.ones_like(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating A and b matrices\n",
    "A_matrix = matrix(y_diag) \n",
    "b_matrix = matrix([0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating G and h matrices\n",
    "G_1 = np.identity(np.shape(y_train)[0])\n",
    "h_1 = C * np.ones_like(y_train)\n",
    "G_2 = -G_1\n",
    "h_2 = 0 * h_1\n",
    "\n",
    "G_matrix = matrix(np.vstack((G_1, G_2)))\n",
    "h_matrix = matrix(np.append(h_1, h_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.81 µs\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3874e+02 -7.3498e+03  4e+04  2e+00  1e-12\n",
      " 1: -7.8159e+01 -3.6059e+03  7e+03  3e-01  1e-12\n",
      " 2: -2.3151e+01 -8.1591e+02  1e+03  6e-02  6e-13\n",
      " 3: -8.2817e+00 -3.2692e+02  5e+02  2e-02  3e-13\n",
      " 4: -1.9531e+00 -5.9043e+01  9e+01  3e-03  7e-14\n",
      " 5: -6.9181e-01 -1.3465e+01  2e+01  7e-04  3e-14\n",
      " 6: -3.3850e-01 -2.8374e+00  3e+00  1e-04  2e-14\n",
      " 7: -3.3182e-01 -1.0866e+00  8e-01  2e-16  2e-14\n",
      " 8: -4.8517e-01 -8.0175e-01  3e-01  2e-16  2e-14\n",
      " 9: -5.4774e-01 -6.8892e-01  1e-01  2e-16  2e-14\n",
      "10: -5.8820e-01 -6.2264e-01  3e-02  2e-16  2e-14\n",
      "11: -6.0131e-01 -6.0673e-01  5e-03  2e-16  2e-14\n",
      "12: -6.0378e-01 -6.0391e-01  1e-04  2e-16  2e-14\n",
      "13: -6.0384e-01 -6.0384e-01  2e-06  2e-16  2e-14\n",
      "14: -6.0384e-01 -6.0384e-01  2e-08  2e-16  2e-14\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Solving the problem\n",
    "sol = solvers.qp(p_matrix, q_matrix, G_matrix, h_matrix, A_matrix, b_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating alpha\n",
    "alpha = np.array(sol['x'])\n",
    "np.where(alpha > 1e-5)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating W\n",
    "y_diag = (y_train.reshape((-1, 1))).T\n",
    "w = (x_train * y_diag) @ alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 2250)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating b\n",
    "x_temp_0 = svm_train_data[(svm_train_data[:, -1] == d), :]\n",
    "x_temp_0 = (np.delete(x_temp_0, -1, axis = 1) / 255).T\n",
    "x_temp_1 = svm_train_data[(svm_train_data[:, -1] == (d + 1) % 10), :]\n",
    "x_temp_1 = (np.delete(x_temp_1, -1, axis = 1) / 255).T\n",
    "x_temp_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum = np.max(w.T @ x_temp_0)\n",
    "minimum = np.min(w.T @ x_temp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = - (maximum + minimum) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 100.0\n"
     ]
    }
   ],
   "source": [
    "# Predicting on test data\n",
    "pred_test = (w.T @ x_test) + b\n",
    "pred_test[pred_test >= 0] = 1\n",
    "pred_test[pred_test < 0] = -1\n",
    "count = 0\n",
    "total = pred_test.shape[1]\n",
    "for (i, p) in enumerate(pred_test[0, :]):\n",
    "    if p == y_test[i]:\n",
    "        count += 1\n",
    "print(\"Accuracy is {}\".format(count * 100 / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 99.8\n"
     ]
    }
   ],
   "source": [
    "# Predicting on validation data\n",
    "pred_val = (w.T @ x_val) + b\n",
    "pred_val[pred_val >= 0] = 1\n",
    "pred_val[pred_val < 0] = -1\n",
    "count = 0\n",
    "total = pred_val.shape[1]\n",
    "for (i, p) in enumerate(pred_val[0, :]):\n",
    "    if p == y_val[i]:\n",
    "        count += 1\n",
    "print(\"Accuracy is {}\".format(count * 100 / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Gaussian\n",
    "gamma = 0.05\n",
    "m = y_train.shape[0]\n",
    "pairwise_dists = squareform(pdist(x_train.T, 'euclidean'))\n",
    "k_matrix = np.exp(- gamma * pairwise_dists ** 2)\n",
    "# x_train[:,i].shape\n",
    "# for i in range(m):\n",
    "#     for j in range(m):\n",
    "#         k_matrix[i, j] = math.exp(-gamma * np.linalg.norm(x_train[:, i] - x_train[:, j]) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing P_matrix\n",
    "y_diag = y_train.reshape((-1, 1))\n",
    "p_matrix = k_matrix\n",
    "p_matrix = y_diag * p_matrix\n",
    "y_diag = y_diag.T\n",
    "p_matrix = p_matrix * y_diag\n",
    "p_matrix = matrix(p_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3464e+02 -6.8388e+03  3e+04  2e+00  2e-15\n",
      " 1: -7.6690e+01 -3.2910e+03  5e+03  2e-01  1e-15\n",
      " 2: -5.4390e+01 -6.8811e+02  8e+02  3e-02  4e-15\n",
      " 3: -8.6653e+01 -2.4599e+02  2e+02  5e-03  2e-15\n",
      " 4: -1.0255e+02 -1.5246e+02  5e+01  1e-03  2e-15\n",
      " 5: -1.0963e+02 -1.2739e+02  2e+01  3e-04  1e-15\n",
      " 6: -1.1318e+02 -1.1820e+02  5e+00  4e-05  1e-15\n",
      " 7: -1.1445e+02 -1.1556e+02  1e+00  4e-06  1e-15\n",
      " 8: -1.1479e+02 -1.1496e+02  2e-01  5e-07  1e-15\n",
      " 9: -1.1485e+02 -1.1486e+02  5e-03  9e-09  2e-15\n",
      "10: -1.1485e+02 -1.1485e+02  1e-04  2e-10  2e-15\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "# Solving the problem\n",
    "sol = solvers.qp(p_matrix, q_matrix, G_matrix, h_matrix, A_matrix, b_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating alpha\n",
    "alpha = np.array(sol['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating B\n",
    "B_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
